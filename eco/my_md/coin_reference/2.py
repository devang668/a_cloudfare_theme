import requests
from bs4 import BeautifulSoup
import re
import json
import time

BASE_URL = "https://www.ouchyi.support"
START_URL = "https://www.ouchyi.support/zh-hans/learn/category/trading-ideas"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def get_all_article_links(start_url):
    """
    æŠ“å–åˆ†ç±»é¡µé¢ä¸­æ‰€æœ‰æ–‡ç« é“¾æ¥ï¼ˆå«åˆ†é¡µè‡ªåŠ¨è¯†åˆ«ï¼‰
    """
    links = set()
    next_page = start_url

    while next_page:
        print(f"ğŸ” æ­£åœ¨æŠ“å–é¡µé¢ï¼š{next_page}")
        res = requests.get(next_page, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # åŒ¹é…æ‰€æœ‰ä»¥ /zh-hans/learn/ å¼€å¤´ä¸”ä»¥ -cn ç»“å°¾çš„æ–‡ç« 
        for a in soup.select("a[href*='/zh-hans/learn/']"):
            href = a.get("href")
            if href and re.search(r"/zh-hans/learn/[^/]+-cn$", href):
                links.add(BASE_URL + href)

        # æ£€æµ‹æ˜¯å¦æœ‰åˆ†é¡µæŒ‰é’®ï¼ˆå¦‚ â€œä¸‹ä¸€é¡µâ€ï¼‰
        next_btn = soup.find("a", string=re.compile("ä¸‹ä¸€é¡µ|Next"))
        if next_btn and next_btn.get("href"):
            next_page = BASE_URL + next_btn["href"]
            time.sleep(1)
        else:
            next_page = None

    print(f"âœ… å…±å‘ç° {len(links)} ç¯‡æ–‡ç« ã€‚")
    return list(links)


def parse_article(url):
    """
    æŠ“å–å•ç¯‡æ–‡ç« å†…å®¹
    """
    print(f"ğŸ“– æŠ“å–æ–‡ç« ï¼š{url}")
    res = requests.get(url, headers=headers, timeout=15)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # æ ‡é¢˜
    title_tag = soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else "æœªçŸ¥æ ‡é¢˜"

    # æ—¥æœŸ
    date = ""
    time_tag = soup.find("time")
    if time_tag:
        date = time_tag.get_text(strip=True)
    else:
        meta_date = soup.find("meta", {"property": "article:published_time"})
        if meta_date and meta_date.get("content"):
            date = meta_date["content"]

    # æ­£æ–‡ï¼ˆå…¼å®¹å¤šç§ç»“æ„ï¼‰
    article_section = (
        soup.find("article")
        or soup.find("div", class_=re.compile("(article-content|learn-article|content-body)"))
    )

    paragraphs = []
    if article_section:
        for p in article_section.find_all(["p", "li"]):
            text = p.get_text(strip=True)
            if text:
                paragraphs.append(text)
    else:
        for p in soup.find_all("p"):
            text = p.get_text(strip=True)
            if text:
                paragraphs.append(text)

    return {
        "title": title,
        "url": url,
        "date": date,
        "content": "\n".join(paragraphs)
    }


def main():
    article_links = get_all_article_links(START_URL)
    results = []

    for i, url in enumerate(article_links, 1):
        try:
            article = parse_article(url)
            results.append(article)
            time.sleep(1)
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ï¼š{url}, é”™è¯¯ï¼š{e}")

    with open("ouchyi_all_articles.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œå…± {len(results)} ç¯‡ï¼Œä¿å­˜ä¸º ouchyi_all_articles.json")


if __name__ == "__main__":
    main()
